\documentclass{article}

\usepackage{mathtools}
\usepackage[margin=1.5in]{geometry}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{xfrac}
\restylefloat{table}

\author{Guillaume Labranche (260585371)}
\title{COMP 531 -- Advanced Theory of Computation\\Assignment \#2}
\date{due on 10 February 2016}

\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\begin{document}

\maketitle

\begin{enumerate}

\item Here we assume that by log-space computable functions, we mean functions which can be computed by a log-space transducer, that is a TM with a read-only input tape, a log-space bounded work tape and a write-only output tape.

\begin{proof} In the trivial solution, for any input $x$ to $f \circ\ g$, we first compute $g(x)$ and then run $f$ on $g(x)$'s output tape. Let $n=|x|$. While $g(x)$ requires logarithmic work space, its output may be polynomial. Let $n' = |g(x)| = O(n^k)$. Then $f(g(x))$ requires $O(\log(n^k)) = O(k \log n) = O(\log n)$ work space. Although individually $f$ and $g$ only require logarithmic space, storing $g(x)$ requires polynomial space. Therefore we devise a trick that enables us to compute $f(g(x))$ without storing $g(x)$.

We modify the transducer $F$ for computing $f$ as follows. Whenever $F$ reads the $i$th symbol of its input tape (which contains $g(x)$), it runs the transducer $G$ for $g$ on $x$ until the $i$th symbol is outputted (transducers have a write-only output tape). This can be done using two counters of size $O(\log(n^k))=O(\log n)$ each, one for storing the position on $F$'s input tape and one for keeping track of the number of symbols outputted by $G$. As we can see, $f \circ g$ is still computable in logarithmic space because we do not store $g(x)$, only one character at a time and two counters of logarithmic size.
\end{proof}

\item The key intuition here is to realize that the son's future \textbf{and} past behaviour is entirely determined by the current edge (and direction) that he is following. With this insight, we can show that he cannot enter a cycle and get stuck in it unless he was spawned in it by his father. This is then used to show that the son always returns to his initial spawn vertex in finite time. Termination and correctness follow from that quite naturally.

\begin{lemma}\label{lemma_deter}
Let $(v,w)\in E$, $(u_1,v,w)$ and $(u_2,v,w)$ be two paths followed by the son at some point during the execution of the algorithm. We claim that $u_1=u_2$.
\end{lemma}
\begin{proof}
Let $(v,w)$ be $v$'s $i$th edge. Therefore by the algorithm behaviour, $(u_1,v)$ is $v$'s $(i-1)$th edge. But $(u_2,v)$ is also $v$'s $(i-1)$th edge. Therefore it must be that $u_1=u_2$.
\end{proof}

\begin{lemma}After being brought to vertex $u$ by his father, the son always returns to $u$ within $2|E|$ time.
\end{lemma}

\begin{theorem}This algorithm detects a cycle $\iff$ there is a cycle.\end{theorem}
\begin{proof}
($\implies$) Consider the only way that the algorithm detects a cycle: When the son is placed at a vertex $v$, leaves through an edge $(v,u)$ and comes back to $v$ through a different edge $(w,v)$ where $u\not=w$. In this case, there is a path from $v$ to $u$, from $u$ to $w$ and from $w$ to $v$. This forms a cycle.

($\Longleftarrow$) When following edge $(v,u)$, the son following the ``cycle searching principle'' always visits all the neighbours of $u$ in sequential order before going back through $(u,v)$. Therefore if the node $v$ is the root of a tree (acyclic graph) the son will perform a tree traversal, coming back along an edge only if the subtree has been been entirely visited. In the case that there is a cycle containing $v$, $u$ and $w$ where $(v,u),(v,w)\in E$, the son will eventually be brought to $v$ by his father. The son will leave $v$ through the edge $(v,u)$ and eventually reach $w$. At that point he will re-enter $v$ and the cycle will be detected.
\end{proof}
\begin{theorem}This algorithm terminates in finite time.\end{theorem}
\begin{proof} (by contradiction) The algorithm evaluates the following condition:

$\forall v\in V, \forall (v,u)\in E$, the son follows $(v,u)$ and comes back to $v$ through $(u,v)$. Since $V$ and $E$ are finite, the only way for the algorithm to not be able to evaluate the condition is if the son does not ever come back to $v$. By the pigeonhole principle, since $V$ is finite the son must follow one edge more than once. The son's behaviour at a particular vertex $v$ is solely dependant on the previous visited vertex and the edges of $v$. Since $G$ does not change during the algorithm's runtime, visiting the same edge in the same direction twice will lead to the exact same behaviour and the son will be stuck in a loop. We will show that such a loop cannot exist.

Let $s\in V$ be a vertex where the son was taken to by his father. Assume that there is a loop in the path taken by the son and let $(v,w)\in E$ be the first edge that he visits twice in his path $P=(s,\dotsc,u_1,v,w,\dotsc,u_2,v,w,\dotsc)$. By lemma \ref{lemma_deter}, $u_1=u_2$ and $(u_1,v)=(u_2,v)$ is a repeated edge visited before $(u,v)$, which contradicts our assumption. Therefore there cannot be such a loop in his path and he will eventually return to $s$.
%When entering a vertex $u$ of fanout $k$, the son will follow edges $(i+1,i+1,$
%When following the edge $(v,u)$, the son follows all other edges of $u$ before coming back through $(v,u)$. 
%The son traverses the graph following the ``cycle searching principle'', starting at every vertex once. Therefore I will show that this principle terminates 
\end{proof}

The algorithm only needs to store a constant amount of vertex indices:
\begin{itemize}
\item which vertex the father is visiting
\item which vertex the son visits initially
\item which vertex the son last visited while traversing the graph
\end{itemize}
Therefore the space complexity is $O(\log |V|)$.

\end{enumerate}
\end{document}
